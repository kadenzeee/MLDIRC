{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e49eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 15:55:40.542012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/u/markhoff/Kitten/lib/python3.11/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy import sparse\n",
    "import awkward as ak\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986c839",
   "metadata": {},
   "source": [
    "# Loading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5ca87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = joblib.load('/u/markhoff/Documents/ML/MLPrtdirc/TPBinnedHist/22TO90DEG.pk1')\n",
    "\n",
    "traintimes  = f['traintimes'][:]\n",
    "valtimes    = f['valtimes'][:]\n",
    "testtimes   = f['testtimes'][:]\n",
    "\n",
    "trainangles = f['trainangles'][:]\n",
    "valangles   = f['valangles'][:]\n",
    "testangles  = f['testangles'][:]\n",
    "\n",
    "trainlabels = f['trainlabels'][:]/2 - 1\n",
    "vallabels   = f['vallabels'][:]/2 - 1\n",
    "testlabels  = f['testlabels'][:]/2 - 1\n",
    "\n",
    "input_dim = traintimes.shape[1] # Assume X.shape = (n_events, n_features), Y.shape = (n_events), then input_dim should be n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b5abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "#\n",
    "#                       PARAMETERS\n",
    "#\n",
    "# ---------------------------------------------------------------\n",
    "class_names = ['Pi+', 'Proton']\n",
    "num_classes = len(class_names) # Pions or kaons?\n",
    "\n",
    "\n",
    "batch_size  = 128 # How many events to feed to NN at a time?\n",
    "nepochs     = 10 # How many epochs?\n",
    "\n",
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d7b2e",
   "metadata": {},
   "source": [
    "# Defining and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4873a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledFiLM(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScheduledFiLM, self).__init__(**kwargs)\n",
    "        self.lambda_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "        self.ranp_rate = 0.01\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, gamma, beta = inputs\n",
    "        lam = tf.clip_by_value(self.lambda_var, 0.0, 1.0)\n",
    "        return (1.0 + lam * gamma) * x + lam * beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9274c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledFiLMCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, film_layer, nepochs):\n",
    "        super(ScheduledFiLMCallback, self).__init__()\n",
    "        self.film_layer = film_layer\n",
    "        self.nepochs = nepochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        new_lambda = (epoch + 1) / self.nepochs\n",
    "        self.film_layer.lambda_var.assign(new_lambda)\n",
    "        print(f'\\nUpdated FiLM lambda to {new_lambda:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f7a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "TypeError: sparse array length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/u/markhoff/Kitten/lib/python3.11/site-packages/scipy/sparse/_base.py\", line 449, in __len__\n    raise TypeError(\"sparse array length is ambiguous; use getnnz()\"\n\nTypeError: sparse array length is ambiguous; use getnnz() or shape[0]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m test_gen    \u001b[38;5;241m=\u001b[39m SparseBatchGenerator(testtimes, testangles, testlabels, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_gen)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_gen\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn [57], line 33\u001b[0m, in \u001b[0;36mSparseBatchGenerator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m batches \u001b[38;5;241m=\u001b[39m [(data[batch_idx]\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, sparse\u001b[38;5;241m.\u001b[39mspmatrix) \u001b[38;5;28;01melse\u001b[39;00m data[batch_idx]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[1;32m     31\u001b[0m batches \u001b[38;5;241m=\u001b[39m [(data[batch_idx]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ak\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;28;01melse\u001b[39;00m data[batch_idx]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(batches[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Kitten/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Kitten/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: TypeError: sparse array length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/u/markhoff/Kitten/lib/python3.11/site-packages/scipy/sparse/_base.py\", line 449, in __len__\n    raise TypeError(\"sparse array length is ambiguous; use getnnz()\"\n\nTypeError: sparse array length is ambiguous; use getnnz() or shape[0]\n\n"
     ]
    }
   ],
   "source": [
    "class SparseBatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Converts sparse matrices to dense batches for training during runtime. Full sparse dataset is loaded into memory, and dense batches are created using this class.\n",
    "    Class keeps ordering and supports shuffling each epoch.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : ndarray, sparseMatrix, awkwardArray \n",
    "        Sparse or dense matrices to be placed into batches. The arguments should include label data as the last argument.\n",
    "    >>> train_gen = SparseBatchGenerator(times, angles, labels, \\*kwargs)\n",
    "    batch_size : int\n",
    "        Number of matrices to batch.\n",
    "    shuffle : bool\n",
    "        Whether or not to shuffle data on epoch end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, batch_size=256, shuffle=True):\n",
    "        self.args = args\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(self.args[0].shape[0])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.ceil(self.args[0].shape[0] / batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batches = [(data[batch_idx].toarray() if isinstance(data, sparse.spmatrix) else data[batch_idx]) for data in self.args]\n",
    "        batches = [(data[batch_idx].to_numpy() if isinstance(data, ak.Array) else data[batch_idx]) for data in self.args]\n",
    "\n",
    "        return batches[:-1], batches[-1]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_gen   = SparseBatchGenerator(traintimes, trainangles, trainlabels, batch_size=batch_size, shuffle=True)\n",
    "val_gen     = SparseBatchGenerator(valtimes, valangles, vallabels, batch_size=batch_size, shuffle=True)\n",
    "test_gen    = SparseBatchGenerator(testtimes, testangles, testlabels, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "_ = len(train_gen)\n",
    "print(train_gen[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e396678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Histogram Branch\n",
    "hist_input = keras.Input(shape=(input_dim,))\n",
    "h = keras.layers.Dropout(0.2)(hist_input)\n",
    "h = keras.layers.Dense(64, activation='relu')(hist_input)\n",
    "\n",
    "#h = keras.layers.Dense(64, activation='relu')(h)\n",
    "\n",
    "\n",
    "# Angle Branch\n",
    "angle_input = keras.Input(shape=(3,))\n",
    "a = keras.layers.Dense(64, activation='relu')(angle_input)\n",
    "#a = keras.layers.Dense(64, activation='relu')(a)\n",
    "\n",
    "# Produce FiLM parameters\n",
    "gamma = keras.layers.Dense(64, activation='linear', name='gamma')(a)\n",
    "beta  = keras.layers.Dense(64, activation='linear', name='beta')(a)\n",
    "gamma = keras.layers.Lambda(lambda g: 1.0 + 1.5*g)(gamma)\n",
    "beta  = keras.layers.Lambda(lambda b: 1.5*b)(beta)\n",
    "\n",
    "# FiLM layer\n",
    "h_mod = keras.layers.Multiply()([h, gamma])\n",
    "h_mod = keras.layers.Add()([h_mod, beta])\n",
    "\n",
    "# Combined layers and output\n",
    "x = keras.layers.Dense(16, activation='relu')(h_mod)\n",
    "out = keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[hist_input, angle_input], outputs=out)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a528a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/markhoff/Kitten/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m val_gen     \u001b[38;5;241m=\u001b[39m SparseBatchGenerator(valtimes, valangles, vallabels, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m test_gen    \u001b[38;5;241m=\u001b[39m SparseBatchGenerator(testtimes, testangles, testlabels, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#callbacks=[ScheduledFiLMCallback(film, nepochs)],\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#test_loss, test_acc = model.evaluate(\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#    test_gen, Y_test, verbose=2\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print('\\nTest accuracy:', test_acc)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print('Test loss:', test_loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/Kitten/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Kitten/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py:124\u001b[0m, in \u001b[0;36m_from_generator\u001b[0;34m(generator, output_types, output_shapes, args, output_signature, name)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_signature):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, type_spec\u001b[38;5;241m.\u001b[39mTypeSpec):\n\u001b[0;32m--> 124\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`output_signature` must contain objects that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclass of `tf.TypeSpec` but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m output_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not."
     ]
    }
   ],
   "source": [
    "#film = ScheduledFiLM()\n",
    "\n",
    "train_gen   = SparseBatchGenerator(traintimes, trainangles, trainlabels, batch_size=batch_size, shuffle=True)\n",
    "val_gen     = SparseBatchGenerator(valtimes, valangles, vallabels, batch_size=batch_size, shuffle=True)\n",
    "test_gen    = SparseBatchGenerator(testtimes, testangles, testlabels, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=nepochs, \n",
    "    #callbacks=[ScheduledFiLMCallback(film, nepochs)],\n",
    ")\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(\n",
    "#    test_gen, Y_test, verbose=2\n",
    "#)\n",
    "\n",
    "#print('\\nTest accuracy:', test_acc)\n",
    "#print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a805cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "accs = []\n",
    "effys = []\n",
    "times = []\n",
    "\n",
    "preds = []\n",
    "\n",
    "for num_nodes in range(2, 17, 1):\n",
    "    print(f'\\n\\nTraining with {num_nodes} nodes in hidden layer\\n')\n",
    "    t1 = time.time()\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(input_dim,)),\n",
    "        keras.layers.Dense(num_nodes, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(num_nodes, activation='relu'),\n",
    "        keras.layers.Dense(num_classes)\n",
    "        ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=nepochs)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "\n",
    "    probability_model = keras.Sequential([model, \n",
    "                                         keras.layers.Softmax()])\n",
    "\n",
    "    predictions = probability_model.predict(test_ds)\n",
    "    confidence = 0.95 # p-value (1 sigma = 0.68, 2 sigma = 0.95, 3 sigma = 0.997, etc)\n",
    "\n",
    "    correct     = 0\n",
    "    total       = 0\n",
    "\n",
    "    Y_test_labels = np.array([y.numpy() for _, y in test_ds.unbatch()])\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        preds.append((np.argmax(pred), np.max(pred)))\n",
    "        if pred[np.argmax(pred)] >= confidence:\n",
    "            total += 1\n",
    "            if np.argmax(pred) == Y_test_labels[i]:\n",
    "                correct += 1\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "\n",
    "    accs.append(correct/total if total > 0 else np.nan)\n",
    "    effys.append(total/predictions.shape[0])\n",
    "    times.append(t2-t1)\n",
    "    \n",
    "    print(f'\\nTest accuracy: {test_acc}, time: {t2-t1} s')\n",
    "\n",
    "print(f'\\n\\nAccuracies: {accs}')\n",
    "print(f'Efficiencies: {effys}')\n",
    "print(f'Times: {times}')\n",
    "print(preds)\n",
    "np.save('predictions.npy', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72515815",
   "metadata": {},
   "source": [
    "# Confidence Filtering\n",
    "\n",
    "- If a prediction for a given event does not reach significance, it is disgarded. Only events which reach significance count towards efficiency. \n",
    "\n",
    "- Significant events that are correct are recorded as a correct prediction. Significant events that are wrong are recorded as an incorrect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict({\"hist_input\": X_hist_test, \"angle_input\": X_angle_test})\n",
    "confidence = 0.003 # p-value (2 sigma = 0.05, 3 sigma = 0.003, etc)\n",
    "\n",
    "correct     = 0\n",
    "total       = 0\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred[np.argmax(pred)] >= 1 - confidence:\n",
    "        total += 1\n",
    "        if np.argmax(pred) == Y_test[i]:\n",
    "            correct += 1\n",
    "\n",
    "try:\n",
    "    print(f'Accuracy: {correct} out of {total}... {correct/total:0.5f}\\nEvents Kept: {total} out of {predictions.shape[0]}... {total/predictions.shape[0]:0.5f}')\n",
    "except ZeroDivisionError:\n",
    "    print('No events kept!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcd5a3",
   "metadata": {},
   "source": [
    "# Test for Dataset\n",
    "\n",
    "- This code checks if there are any duplicate events in the dataset, and gives the confusion matrix of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_corr(X_hist, X_angle, Y, name=\"Set\"):\n",
    "    X_all = np.concatenate([X_hist, X_angle], axis=1)\n",
    "    df = pd.DataFrame(X_all)\n",
    "    df['label'] = Y\n",
    "    corr = df.corr()['label'].abs().sort_values(ascending=False)\n",
    "    print(f\"\\nTop correlations with label in {name}:\")\n",
    "    print(corr.head(10))\n",
    "\n",
    "\n",
    "\n",
    "quick_corr(X_hist_train[:2], X_angle_train[:2], Y_train[:2], \"Train\")\n",
    "quick_corr(X_hist_val[:2], X_angle_val[:2], Y_val[:2], \"Val\")\n",
    "quick_corr(X_hist_test[:2], X_angle_test[:2], Y_test[:2], \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in np.unique(Y_train):\n",
    "    plt.hist(X_angle_train[Y_train == cls, 2], bins=50, alpha=0.5, label=f'Class {int(cls)}')\n",
    "plt.xlabel('Angle Feature 0')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Angle Feature 2 by Class (Train)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70121a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "probability_model = keras.Sequential([model, keras.layers.Softmax()])\n",
    "predictions = probability_model.predict({\"hist_input\": X_hist_test, \"angle_input\": X_angle_test})\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(np.arccos(X_angle_test.T[2]).min()*180/np.pi)\n",
    "\n",
    "X_angle_test = np.arccos(X_angle_test)*180/np.pi\n",
    "\n",
    "# Bin by angle (e.g., first angle feature)\n",
    "angle_bins = np.linspace(X_angle_test.T[2].min(), X_angle_test.T[2].max(), 10)\n",
    "digitized = np.digitize(X_angle_test[:,2], angle_bins)\n",
    "acc_by_bin = [np.mean(y_pred[digitized == i] == Y_test[digitized == i]) for i in range(1, len(angle_bins))]\n",
    "\n",
    "plt.plot(angle_bins[1:], acc_by_bin, marker='o')\n",
    "plt.xlabel('Angle Feature 0')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy vs Angle Feature 0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix on Test Set ---\n",
    "probability_model = keras.Sequential([model, keras.layers.Softmax()])\n",
    "predictions = probability_model.predict({\"hist_input\": X_hist_test, \"angle_input\": X_angle_test})\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = Y_test.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kitten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
